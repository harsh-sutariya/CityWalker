<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos">
  <meta name="keywords" content="Visual navigation, Embodied navigation, Video Learning">     
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CityWalker</title>
  <link rel="icon" type="image/png" href="./src/logo.png">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/copy2clipboard.js"></script>
  <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
  
  <script src="https://cdn.knightlab.com/libs/juxtapose/latest/js/juxtapose.min.js"></script>
  <link rel="stylesheet" href="https://cdn.knightlab.com/libs/juxtapose/latest/css/juxtapose.css">
</head>


<body>

<!-- Navigation bar. -->
<nav class="navbar is-light" role="navigation" aria-label="main navigation">
  <div class="container is-max-desktop">

    <!-- Lab logo. Will stay here even if view from mobile -->
    <div class="navbar-brand">
      <a class="navbar-item" href="https://ai4ce.github.io/" target="_blank" rel="noopener noreferrer">
        <img src="./static/images/ai4ce_new_linear_notext.svg" alt="AI4CE Lab" style="height: 2.0rem;">
      </a>
      <a role="button" onclick="this.classList.toggle('is-active');document.querySelector('#'+this.dataset.target).classList.toggle('is-active');" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <!-- Will collapse into a "burger" menu on mobile. -->
    <div id="navbarBasicExample" class="navbar-menu">

      <div class="navbar-start">
        <a class="navbar-item" href="https://www.nyu.edu/" target="_blank">
          <img src="./static/images/NYU_Long_RGB_Color.png" alt="NYU Logo" style="height: 2.0rem;">
        </a>
      </div>
      

      <div class="navbar-end">
        <!-- After accepted, add conference logo here -->
        <!-- <a class="navbar-item" href="https://cvpr.thecvf.com/Conferences/2024" target="_blank" rel="noopener noreferrer">
            <img src="img/logo/logo-cvpr.png" alt="CVPR 2024" style="height: 2.0rem;">
        </a> -->

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://ai4ce.github.io/DeepExplorer/">
              DeepExplorer
            </a>
            <a class="navbar-item" href="https://ai4ce.github.io/MSG/">
              Multiview Scene Graph
            </a>
          </div>
        </div>

      </div>
      
    </div>

  </div>
</nav>

<!-- Title and authors. -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/logo.png" alt="CityWalker Icon" style="vertical-align: middle; height: 10em; margin-right: 0.1em;">
          <h1 class="title is-2 publication-title">
              CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos
          </h1>
          <div class="column is-full_width">
            <h2 class="title is-4"><a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a></h2>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://gaaaavin.github.io/">Xinhao Liu*</a>,
            </span>
            <span class="author-block">
              <a href="https://ai4ce.github.io/CityWalker/">Jintong Li*</a>,
            </span>
            <span class="author-block">
              <a href="https://ai4ce.github.io/CityWalker/">Yicheng Jiang</a>,
            </span>
            <span class="author-block">
              <a href="https://ai4ce.github.io/CityWalker/">Niranjan Sujay</a>,
            </span>
            <span class="author-block">
              <a href="https://ai4ce.github.io/CityWalker/">Zhicheng Yang</a>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://juexzz.github.io/">Juexiao Zhang</a>,
            </span>
            <span class="author-block">
              <a href="https://ai4ce.github.io/CityWalker/">John Abanes</a>,
            </span>
            <span class="author-block">
              <a href="https://jingz6676.github.io//">Jing Zhang</a>,
            </span>
            <span class="author-block">
              <a href="https://engineering.nyu.edu/faculty/chen-feng">Chen Feng&dagger;</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"> New York University</span>
            <br>
            <span class="author-block"><sup>*</sup> Equal contribution </span>
            <span class="author-block"><sup>&dagger;</sup> Corresponding author </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                 <!-- add here later. -->
                <a href="https://arxiv.org/abs/2411.17820"                
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- <span class="link-block"> -->
                <!-- add here later. -->
               <!-- <a href=""                    
                  class="external-link button is-normal is-rounded is-dark">
                 <span class="icon">
                     <i class="fas fa-camera"></i>
                 </span>
                 <span>Appendix</span>
               </a> -->
             <!-- </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/thC0PeAQxe0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ai4ce/CityWalker"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ai4ce/CityWalker"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width teaser no-margin">

        <h2 class="title is-3">TLDR</h2>
        <p style="font-size: 20px; background-color: #8b00e13b;">
          We leverage thousands of hours of online city walking and driving videos to train autonomous agents for robust,
          generalizable navigation in urban environments through scalable, data-driven imitation learning.
        </p>
        <br>
        <video muted autoplay loop controls controls id="videoPlayer">
          <source src="./static/videos/CityWalker_demo_anonymous_480p.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>

<hr>

<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Navigating dynamic urban environments presents significant challenges for embodied agents, requiring advanced spatial
            reasoning and adherence to common-sense norms. Despite progress, existing visual navigation methods struggle in map-free
            or off-street settings, limiting the deployment of autonomous agents like last-mile delivery robots. To overcome these
            obstacles, we propose a scalable, data-driven approach for <b>human-like urban navigation</b> by training agents on thousands
            of hours of in-the-wild city walking and driving videos sourced from the web. We introduce a simple and scalable data
            processing pipeline that extracts action supervision from these videos, enabling large-scale imitation learning without
            costly annotations. Our model learns sophisticated navigation policies to handle diverse challenges and critical
            scenarios. Experimental results show that training on large-scale, diverse datasets significantly enhances navigation
            performance, surpassing current methods. This work shows the potential of using abundant online video data to develop
            robust navigation policies for embodied agents in dynamic urban settings.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/1teasing.gif" alt="CityWalker Teasing" style="height: 20em;">
      </div>
    </div>
  </div>
</section>

<hr>

<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <center>
          <!-- This image is always visible  -->
          <div id="overview">
            <img src="./static/images/2pipeline.png" style="width: 45vw; min-width: 330px;" alt="Pipeline">
          </div>

          <div class="content has-text-justified">
            <p id="text-content"> <b>Overall Pipeline of CityWalker</b>.
            Our training pipeline starts with internet-sourced videos, using visual odometry to obtain
            relative poses between frames. At each time step, the model receives past observations, past trajectory, and target
            location as input. They
            are encoded via a frozen image encoder and a trainable coordinate encoder. A transformer processes these inputs to
            generate future tokens.
            An action head and an arrival head decode these tokens into action and arrival status predictions. During training,
            future frame tokens from
            future frames guide the transformer to hallucinate future tokens.
            </p>
          </div>
        </center>
      </div>
    </div>
  </div>
</section>

<hr>

<!-- Dataset. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width teaser no-margin">

        <h2 class="title is-3">Dataset</h2>
        <video muted autoplay loop controls controls id="videoPlayer">
          <source src="./static/videos/mosaic_h264.mp4" type="video/mp4">
        </video>
        <div class="content has-text-justified">
          <p id="text-content">
            Our dataset incoporates more than 2000 hours of websourced city walking and driving videos. 
            Above is a mosaic of a small subset
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>

<!-- Results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Experiment Results</h2>
        <center>
          <!-- This image is always visible  -->
          <div id="overview">
            <img src="./static/images/8scale.png" style="width: 45vw; min-width: 330px;" alt="Scaling Results">
          </div>

          <div class="content has-text-justified">
            <p id="text-content"> <b>Performance and Data Scale</b>.
              We show the model
              performance evaluated by MAOE with respect to the size of the
              training data measured by video length in hours. We also show
              the zero-shot performance of our model trained with only driving
              videos and mixed driving and walking videos
            </p>
          </div>
        </center>
      </div>
    </div>
  </div>
</section>

<hr>

<!-- Visualization -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
        <center>
          <!-- This image is always visible  -->
          <div id="overview">
            <img src="./static/images/11more_qual.png" alt="Qualitative Results">
          </div>

          <div class="content has-text-justified">
            <p id="text-content"> <b>Qualitative Results</b>.
              We divide the results into three categories. Success: predicted action aligns well with ground truth action.
              Large error: predicted action does not align with ground
              truth but may still lead to successful navigation. Fail: predicted action may lead to failed navigation. The most
              significant observation is that large errors in offline data do not
              necessarily lead to failure in navigation, due to the multimodality characteristic of policy learning. For example, in
              the fifth row, although the ground truth action takes a detour to the right of the traffic drum, the predicted action
              that
              goes straight from the left of the drum should also lead to
              successful navigation.
            </p>
          </div>
        </center>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX"> 
  <div class="container is-max-desktop content">
    <center>
    <h2 id="bibtexTitle" class="title">BibTeX</h2>
    <button id="copyButton" onclick="copyToClipboard()">
      <i class="fas fa-copy"></i>
    </button>
    <br>
    <pre style="display: inline-flex; text-align: left";><code id="bibtexInfo">
@article{liu2024citywalker,
title={CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos},
author={Liu, Xinhao and Li, Jintong and Jiang, Yicheng and Sujay, Niranjan and Yang, Zhicheng and Zhang, Juexiao and
Abanes, John and Zhang, Jing and Feng, Chen},
journal={arXiv preprint arXiv:2411.17820},
year={2024}
}
      </code>
    </pre>
  </center>
  </div>
</section>
          
<!-- Acknowledgements   -->
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    The work was supported by NSF grants 2238968, 2121391, 2322242 and 2345139; and in part through the NYU IT High
    Performance Computing resources, services, and staff expertise. We also thank Xingyu Liu and Zixuan Hu for their help in
    data collection.
  </div>
</section>
          
<!-- Footer -->       
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.<br>
            This AI4CE template is created by <a href="https://irvingf7.github.io/">Irving Fang</a>.<br>
            This webpage template is originally from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
            This website is then inspired by the project page of <a href="https://cat3d.github.io/">CAT3D</a>, <a href="https://armlabstanford.github.io/touch-gs">Touch-GS</a> and <a href="https://baegwangbin.github.io/DSINE/">DSINE</a>.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
  </div>
</footer>

</body>

</html>
